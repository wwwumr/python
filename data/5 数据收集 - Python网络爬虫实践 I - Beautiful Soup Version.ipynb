{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;数据科学引论 - Python之道 </h1>\n",
    "\n",
    "<h1 align=center> &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;第5课 数据收集 - Python网络爬虫实践 I </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 爬虫概述\n",
    "在阅读这个样例之前，建议先了解爬虫是什么，简单理解url、爬虫技术、网页html等基本概念，这可以参考链接http://python.jobbole.com/81334/\n",
    "\n",
    "本笔记本所依赖Python爬虫Beautiful Soup，大家可以通过命令 *pip3 install beautifulsoup4* 安装所需依赖包。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 定义爬虫的任务\n",
    "\n",
    "## 涉及的语法\n",
    "语法涉及类（面向对象）、列表list、字典dict、循环、函数、字符串操作、文件读写\n",
    "\n",
    "## 概述\n",
    "这个爬虫的任务是爬取http://quotes.toscrape.com/page/1/ 的前两页，提取每条名言的文字内容，作者和标签，最后以JSON格式保存到文件中\n",
    "\n",
    "\n",
    "## 如何修改\n",
    "\n",
    "在自己做定制时，只需要修改`__init__`和`parse`两个方法，通俗讲__init__方法决定了爬取哪些网站，parse则指明了在每一个网页上爬取哪些内容\n",
    "- init_urls: 设置待爬取网站的列表和保存文件路径，其中变量self.urls是待爬取网站的列表，self.file是一个文件对象\n",
    "- parse：方法内是针对每个url成功访问之后进行的页面解析\n",
    "   关于如何解析具体网页，可根据实际需要查看Beautiful Soup的官方文档 https://www.crummy.com/software/BeautifulSoup/bs4/doc.zh/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "\n",
    "class MySpider():\n",
    "    \n",
    "    name = \"spider\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.file = open('demo1_quotes_bs.json', 'w');\n",
    "        \n",
    "        #设置待爬取网站列表\n",
    "        self.urls = []\n",
    "        for i in range(1,3):\n",
    "            self.urls.append('http://quotes.toscrape.com/page/' + str(i) )\n",
    "            \n",
    "#       初始化效果 效果等同\n",
    "#         self.urls = [\n",
    "#             'http://quotes.toscrape.com/page/1/',\n",
    "#             'http://quotes.toscrape.com/page/2/',\n",
    "#         ]\n",
    "\n",
    "        # 设置header\n",
    "        self.headers = {'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'}\n",
    "\n",
    "        \n",
    "        print(self.urls)\n",
    "        \n",
    "    \n",
    "    # 利用urllib获取网站的html，并导入BeautifulSoup\n",
    "    def bs_request(self, url):\n",
    "        request = urllib.request.Request(url, headers=self.headers)\n",
    "        html = urllib.request.urlopen(request).read()\n",
    "        response = BeautifulSoup(html,'html.parser')\n",
    "        self.parse(url,response)\n",
    "        \n",
    "    # start函数，调用此函数可开始爬虫   \n",
    "    def start(self):\n",
    "        for url in self.urls:\n",
    "            self.bs_request(url)\n",
    "    \n",
    "\n",
    "    # parse方法用于解析html文件\n",
    "    def parse(self, url, response):\n",
    "        \n",
    "        #提取名言列表\n",
    "        quotes = response.find_all(\"div\", class_=\"quote\")\n",
    "        for quote in quotes:\n",
    "            #提取每条名言中的作者名\n",
    "            author = quote.find(\"small\", class_=\"author\").get_text()\n",
    "            #提取名言的文字内容\n",
    "            text = quote.find(class_=\"text\").get_text()\n",
    "            #提取名言标签\n",
    "            tags = [t.get_text() for t in quote.select(\".tags .tag\")]\n",
    "            #构建字典对象\n",
    "            item = {\"author\":author, \"text\": text, \"tags\":tags }\n",
    "            #将字典转换成json字符串\n",
    "            line = json.dumps(dict(item))\n",
    "            #将每个条目写入文件\n",
    "            self.file.write(line + \"\\n\")\n",
    "        #及时将内容写入文件，否则可能会出现少许延迟\n",
    "        self.file.flush()\n",
    "        os.fsync(self.file)\n",
    "        #输出当前解析完成的网页网址，可以当做爬取进度来看待,与程序逻辑无关\n",
    "        print(\"over: \" + url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['http://quotes.toscrape.com/page/1', 'http://quotes.toscrape.com/page/2']\n",
      "over: http://quotes.toscrape.com/page/1\n",
      "over: http://quotes.toscrape.com/page/2\n"
     ]
    }
   ],
   "source": [
    "# 新建爬虫对象\n",
    "spider = MySpider()\n",
    "\n",
    "# 开始爬虫\n",
    "spider.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
